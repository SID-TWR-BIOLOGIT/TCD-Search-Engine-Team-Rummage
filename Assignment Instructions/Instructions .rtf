{\rtf1\ansi\ansicpg1252\cocoartf1671
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl340\partightenfactor0

\f0\b\fs28\fsmilli14080 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Assignment 2 - Detailed Specification\
\pard\pardeftab720\sl280\partightenfactor0

\f1\b0\fs24\fsmilli12160 \cf2 \cb3 Posted on: 22 October 2018 12:32:24 o'clock BST\cb1 \
\
\pard\pardeftab720\sl300\sa260\partightenfactor0

\fs26 \cf2 \cb3 Hi all,\cb1 \uc0\u8232 \u8232 \cb3 The second assignment uses, and builds upon the knowledge you have acquired in the first assignment.\cb1 \uc0\u8232 \cb3 The task is again to use Lucene to index and search a specific collection of documents.\'a0\cb1 \uc0\u8232 \cb3 This time it is a group-based assignment, with teams working together to design and build a search engine that performs as well as possible on the given collection. Class bragging rights are at stake, who will be the 2018 CS7IS3 champions?!\'a0\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Content
\f1\b0 \cb1 \uc0\u8232 \cb3 The content in question is an aggregated collection of news articles (mostly) from a number of sources: the Financial Times Limited (1991, 1992, 1993, 1994), the Federal Register (1994), the Foreign Broadcast Information Service (1996) and the Los Angeles Times (1989, 1990). You can download the full content collection from here - https://drive.google.com/a/tcd.ie/file/d/1MudJity9Ckh8jxapFx3OS-DLEkcvbYYx/view?usp=drive_web\cb1 \uc0\u8232 \cb3 There is a README file in the top level folder which explains the content collection and how the folders are organised.\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Indexing
\f1\b0 \cb1 \uc0\u8232 \cb3 From your experience in assignment 1, you already know the basics of how to index a collection. It is now time to start to think in more depth about the indexing process.\'a0\cb1 \uc0\u8232 \cb3 What fields (if any) should the parsed documents be separated into? What stop word removal, stemming, phrase identification and other analysers should be used? Are there more complex linguistic, modelling or indexing processes that could be used?\'a0\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Queries\'a0
\f1\b0 \cb1 \uc0\u8232 \cb3 In assignment one the queries were simple, pre-defined short strings of text.\'a0\cb1 \uc0\u8232 \cb3 In many IR Evaluation tasks, however, instead of a defined query, you are given a "topic". Each topic represents a user's information need. An example topic is below:\cb1 \uc0\u8232 \u8232 \cb3 <top>\cb1 \uc0\u8232 \cb3 <num> Number: 404\'a0\'a0\cb1 \uc0\u8232 \cb3 <title> Ireland, peace talks\'a0\cb1 \uc0\u8232 \cb3 <desc> Description:\'a0 How often were the peace talks in Ireland delayed\'a0 or disrupted as a result of acts of violence?\cb1 \uc0\u8232 \cb3 <narr> Narrative:\'a0 Any interruptions to the peace process not directly\'a0 attributable to acts of violence are not relevant.\cb1 \uc0\u8232 \cb3 </top>\cb1 \uc0\u8232 \u8232 \cb3 In this collection, binary relevance judgements are made against these topics, rather than against specified queries.\'a0\cb1 \uc0\u8232 \cb3 In other words, documents in the collection are judged based upon whether they satisfy the underlying information need.\'a0\cb1 \uc0\u8232 \cb3 The topics for assignment two can be found here - https://www.dropbox.com/s/277vn6l23z2e6ku/CS7IS3-Assignment2-Topics.gz?dl=0\'a0\cb1 \uc0\u8232 \cb3 You need to take these topics and automatically generate a query (or queries) that best represents the information need described in the topic, and returns the most relevant results. You can experiment with a number of approaches, just using the title, using the description, using a combination of all three fields, trying to identify the best information carrying words in the topic and generating a query using those. You can also experiment with more advanced approaches such as query expansion.\cb1 \uc0\u8232 \cb3 Whatever approach you decide upon, should generate one set of results per topic, the best 1000 documents for that topic.\'a0\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Retrieval Model
\f1\b0 \'a0\cb1 \uc0\u8232 \cb3 The retrieval model used can significantly impact the performance of your search engine. You should experiment with a variety of retrieval models to identify which performs best for this collection and these topics. You should also consider if the retrieval models could be extended in any way to improve the performance of your search engine for this content.\'a0\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Group Work
\f1\b0 \cb1 \uc0\u8232 \cb3 Each group must keep a Project Journal\'a0\cb1 \uc0\u8232 \cb3 This journal should be on a Google Doc, and you should share access to this doc with the TA team - using CS7IS3TCD@gmail.com\'a0\cb1 \uc0\u8232 \cb3 The journal should document all project activity and each team member's contributions in brief summary form: i.e. the date of each meeting, the name of participants, the role of each participant, a bullet point summary of the meeting, the contribution of each participant to implementation, the contribution of each participant to the research paper.\cb1 \uc0\u8232 \cb3 This journal is mandatory, failure to provide the document from the outset of the assignment will result in a penalty.\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Project Phases
\f1\b0 \cb1 \uc0\u8232 \cb3 There will be two development phases and a write-up phase:\cb1 \uc0\u8232 \cb3 - Development Phase 1 - October 22nd - November 14th\'a0\cb1 \uc0\u8232 \cb3 - Development Phase 2 - November 15th - November 28th\'a0\cb1 \uc0\u8232 \cb3 - Write-up Phase - October 22nd - November 30th\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Phase 1
\f1\b0 \cb1 \uc0\u8232 \cb3 On the 14th of November, the end of the first development phase, we will upload a QRel file containing relevance judgements for half of the topics to your AWS instance. We will then use TREC_eval to see how well your search engines are performing for those topics. We will generate a class league table at this point to show how well the various group's search engines are performing. The most important metric will be Mean Average Precision.\'a0\cb1 \uc0\u8232 \cb3 So what you need by the 14th is:\cb1 \uc0\u8232 \cb3 - Code available on the instance that builds a Lucene index, generates and executes your queries against the index and produces 1000 result per topic\cb1 \uc0\u8232 \cb3 - The latest version of trec_eval installed so that we can generate evaluation scores for your generated results. Note that the DOCNO field is what will be matched against the QRels (rather than DOCID).\'a0\cb1 \uc0\u8232 \cb3 - Access details for your instance and instructions on running your code and where to find your queries. One member of your team can email these details to CS7IS3TCD@gmail.com\cb1 \uc0\u8232 \cb3 - Note: there will be no grading at this point, it is just an opportunity for teams to assess how their search engine is performing and adjust accordingly.\'a0\cb1 \uc0\u8232 \cb3 It will be important at that point not to overfit your search engine to these topics based upon these results, remember these are just half the topics and relevance judgements, your search engine must still perform well for the other topics.\'a0\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Phase 2
\f1\b0 \cb1 \uc0\u8232 \cb3 At the end of the second development phase, once you have submitted your final systems, the remaining half of the QRels will be released. We will test everyone's submitted application at this point using all the relevance judgements and generate the final evaluation scores for each team and the final class league table. At this point you will have a couple of days to incorporate the final evaluation scores into your report before submission.\'a0\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Multiple Runs
\f1\b0 \cb1 \uc0\u8232 \cb3 It is often the case in IR evaluation tasks that a number of different combinations of search engine components may be suited to the task in question. To allow for this, each group can, if they choose, submit two "runs" (i.e. two different Lucene implementations that use different combinations of indexing, analysis, query generation and/or retrieval model).\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Write-up phase
\f1\b0 \cb1 \uc0\u8232 \cb3 This phase runs concurrently with the development phases, with a submission deadline of 30th of November. Each team must write a research paper in the style of a SIGIR short paper - http://sigir.org/sigir2018/submit/call-for-short-papers/ i.e. can not exceed four pages in the current ACM two-column conference format (including references and figures).\cb1 \uc0\u8232 \cb3 You should describe the approach your team took, your implementation, focusing of what you did above and beyond a standard Lucene install. You must reference any relevant research papers that you have read on the topic, especially those whom you have based your approach on. If your team creates a novel approach the builds upon the state of the art, there will be the opportunity to submit a refined version of this paper to the SIGIR conference in early 2019. If accepted, this research paper would be a significant achievement.\'a0\cb1 \uc0\u8232 \cb3 This report will be graded based upon:\cb1 \uc0\u8232 \cb3 1. Novelty and Appropriateness of the Proposed Approach\cb1 \uc0\u8232 \cb3 2. Technical Soundness\'a0\cb1 \uc0\u8232 \cb3 3. Clarity of Presentation, Structure, Grammar/Spelling, Style etc.\'a0\cb1 \uc0\u8232 \cb3 4. Results Presentation\cb1 \uc0\u8232 \cb3 5. Honest, open reflection upon the success and limitations of the proposed approach\cb1 \uc0\u8232 \cb3 6. Adequacy of Bibliography\'a0\cb1 \uc0\u8232 \cb3 Safe Assign will be used and plagiarism will not be tolerated.\'a0\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Submission
\f1\b0 \cb1 \uc0\u8232 \cb3 This will follow the same format as assignment one.\cb1 \uc0\u8232 \cb3 The research paper will be submitted through Blackboard (mymodule.tcd.ie). One member of your team should submit the report on behalf of the team - it is not necessary for all team members to upload the same report.\cb1 \uc0\u8232 \cb3 The code should be made available to the TA team on an AWS instance. Again, one AWS instance per team (with two lucent builds in the team wishes) is all that is needed, not one per person.\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Advice
\f1\b0 \cb1 \uc0\u8232 \cb3 People have used this collection before and developed approaches to tackle the challenges presented by the collection - read research papers, look at approaches that have been used successfully in the past. It is fine to reuse or reimplement existing research that has been shown to be successful, however, you must reference this work, explain why you selected it, and your team must be able to explain every line of code when submitted.\cb1 \uc0\u8232 \u8232 
\f0\b \cb3 Groups
\f1\b0 \cb1 \uc0\u8232 \cb3 You will all be emailed the membership of your group shortly.\cb1 \uc0\u8232 \u8232 \cb3 Good luck!!\cb1 \uc0\u8232 \u8232 \cb3 S\'e9amus and the TA Team\cb1 \
}